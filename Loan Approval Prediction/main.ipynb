{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Loan Approval Prediction: A Machine Learning Pipeline\n",
    "\n",
    "This notebook demonstrates a complete end-to-end machine learning pipeline for predicting loan approval status. We traverse through data acquisition, pre-processing, feature engineering, and model evaluation using **scikit-learn** and **pandas**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "header-1",
   "metadata": {},
   "source": [
    "## 1. Data Acquisition & Initial Setup\n",
    "\n",
    "In this phase, we load binary libraries and the raw data. We also perform initial cleaning to ensure the data is in a usable format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "context-1",
   "metadata": {},
   "source": [
    "### Importing Core Libraries\n",
    "\n",
    "**What is happening:** Loading the fundamental data science stack.\n",
    "\n",
    "**The \"Why\":** We utilize **pandas** and **numpy** for data manipulation, while **matplotlib** and **seaborn** handle visualizations. **scikit-learn** provides the necessary tools for pre-processing, splitting, and modeling.\n",
    "\n",
    "**Best Practices:** Grouping all imports at the top of the notebook ensures clear dependency management and improves readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "187de248",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "context-2",
   "metadata": {},
   "source": [
    "### Loading and Cleaning the Dataset\n",
    "\n",
    "**What is happening:** Reading the CSV file and removing unnecessary columns.\n",
    "\n",
    "**The \"Why\":** Removing trailing spaces from column names ensures consistent indexing. Dropping `loan_id` is essential as unique identifiers do not contribute to the model's predictive power.\n",
    "\n",
    "**Best Practices:** Identifying and removing unique ID columns prevents the model from 'memorizing' specific rows, which helps in better generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "24b8eebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('loan_approval_dataset.csv')\n",
    "\n",
    "# Clean column names by stripping whitespace\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# Remove unique identifiers\n",
    "df = df.drop(columns=['loan_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "header-2",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis (EDA)\n",
    "\n",
    "EDA is a critical step to understand the distribution of the target variable and the statistical properties of the features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "context-3",
   "metadata": {},
   "source": [
    "### Analyzing Target Class Proportions\n",
    "\n",
    "**What is happening:** Calculating the percentage of approved vs. rejected loans.\n",
    "\n",
    "**The \"Why\":** Understanding class balance is crucial. If one class significantly outweighs the other, accuracy can be a misleading metric.\n",
    "\n",
    "**Best Practices:** Check class proportions early to determine if sampling techniques (like SMOTE) or adjusted evaluation metrics are needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "e2c39a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proportion:\n",
      "class 1: 62.22\n",
      "class 2: 37.78\n"
     ]
    }
   ],
   "source": [
    "count = np.array(df.value_counts(['loan_status']))\n",
    "print('proportion:')\n",
    "print(f'class 1: {100 * count[0] / np.sum(count):.2f}')\n",
    "print(f'class 2: {100 * count[1] / np.sum(count):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "context-4",
   "metadata": {},
   "source": [
    "### Inspecting Feature Names\n",
    "\n",
    "**What is happening:** Printing the current column index.\n",
    "\n",
    "**The \"Why\":** Verifying feature names ensures we are tracking all variables correctly after the initial cleanup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "a4eef4f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['no_of_dependents', 'education', 'self_employed', 'income_annum',\n",
      "       'loan_amount', 'loan_term', 'cibil_score', 'residential_assets_value',\n",
      "       'commercial_assets_value', 'luxury_assets_value', 'bank_asset_value',\n",
      "       'loan_status'],\n",
      "      dtype='object') 12\n"
     ]
    }
   ],
   "source": [
    "print(df.columns, len(df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "context-5",
   "metadata": {},
   "source": [
    "### Statistical Summary of Numerical Features\n",
    "\n",
    "**What is happening:** Generating summary statistics (mean, std, min, max) for numerical variables.\n",
    "\n",
    "**The \"Why\":** This helps identify the range of values for each feature, which is vital for deciding the scaling strategy.\n",
    "\n",
    "**Best Practices:** Look for anomalies, such as negative values in asset valuations, that might indicate data entry errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "b29247c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       no_of_dependents  income_annum   loan_amount    loan_term  cibil_score  \\\n",
       "count       4269.000000  4.269000e+03  4.269000e+03  4269.000000  4269.000000   \n",
       "mean           2.498712  5.059124e+06  1.513345e+07    10.900445   599.936051   \n",
       "std            1.695910  2.806840e+06  9.043363e+06     5.709187   172.430401   \n",
       "min            0.000000  2.000000e+05  3.000000e+05     2.000000   300.000000   \n",
       "25%            1.000000  2.700000e+06  7.700000e+06     6.000000   453.000000   \n",
       "50%            3.000000  5.100000e+06  1.450000e+07    10.000000   600.000000   \n",
       "75%            4.000000  7.500000e+06  2.150000e+07    16.000000   748.000000   \n",
       "max            5.000000  9.900000e+06  3.950000e+07    20.000000   900.000000   \n",
       "\n",
       "       residential_assets_value  commercial_assets_value  luxury_assets_value  \\\n",
       "count              4.269000e+03             4.269000e+03         4.269000e+03   \n",
       "mean               7.472617e+06             4.973155e+06         1.512631e+07   \n",
       "std                6.503637e+06             4.388966e+06         9.103754e+06   \n",
       "min               -1.000000e+05             0.000000e+00         3.000000e+05   \n",
       "25%                2.200000e+06             1.300000e+06         7.500000e+06   \n",
       "50%                5.600000e+06             3.700000e+06         1.460000e+07   \n",
       "75%                1.130000e+07             7.600000e+06         2.170000e+07   \n",
       "max                2.910000e+07             1.940000e+07         3.920000e+07   \n",
       "\n",
       "       bank_asset_value  \n",
       "count      4.269000e+03  \n",
       "mean       4.976692e+06  \n",
       "std        3.250185e+06  \n",
       "min        0.000000e+00  \n",
       "25%        2.300000e+06  \n",
       "50%        4.600000e+06  \n",
       "75%        7.100000e+06  \n",
       "max        1.470000e+07  "
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "header-3",
   "metadata": {},
   "source": [
    "## 3. Data Pre-processing\n",
    "\n",
    "This section involves splitting the dataset and encoding the target variable to prepare for model ingestion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "context-6",
   "metadata": {},
   "source": [
    "### Splitting Data into Training and Testing Sets\n",
    "\n",
    "**What is happening:** Dividing features (X) and target (Y) into train and test subsets.\n",
    "\n",
    "**The \"Why\":** A 20% test split allows us to evaluate the model's accuracy on unseen data.\n",
    "\n",
    "**Best Practices:** We encode the target variable using **OneHotEncoder** with `drop='if_binary'` to create a single binary column and prevent redundancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "9c340df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df = df.iloc[:, :-1]\n",
    "Y_df = df.iloc[:, -1]\n",
    "\n",
    "# Encode the target variable (Binary Loan Status)\n",
    "Y_encoder = OneHotEncoder(drop='if_binary', sparse_output=False)\n",
    "Y = Y_encoder.fit_transform(Y_df.to_frame())\n",
    "\n",
    "# Split into training (80%) and testing (20%) sets\n",
    "train_X_df, test_X_df, train_Y, test_Y = train_test_split(X_df, Y, test_size=.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "context-7",
   "metadata": {},
   "source": [
    "### Separating Numerical and Categorical Features\n",
    "\n",
    "**What is happening:** Isolating different data types for specialized processing.\n",
    "\n",
    "**The \"Why\":** Machine learning algorithms generally require numerical input, but categorical variables first need translation via encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "a3c259bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_numerical = train_X_df.select_dtypes(include=['number'])\n",
    "train_X_categorical = train_X_df.select_dtypes(exclude=['number'])\n",
    "\n",
    "test_X_numerical = test_X_df.select_dtypes(include=['number'])\n",
    "test_X_categorical = test_X_df.select_dtypes(exclude=['number'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "header-4",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering\n",
    "\n",
    "We apply transformations to numerical and categorical features to ensure optimal model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "context-8",
   "metadata": {},
   "source": [
    "### Feature Scaling and Encoding\n",
    "\n",
    "**What is happening:** Standardizing numerical features and One-Hot encoding categorical ones.\n",
    "\n",
    "**The \"Why\":** Standardization (calculating $z = \\frac{x - \\mu}{\\sigma}$) ensures that high-magnitude features like 'loan_amount' don't overpower smaller ones like 'no_of_dependents'.\n",
    "\n",
    "**Best Practices:** Scale only on the training set to prevent **Data Leakage** into the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "6e7bd36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale numerical features using Z-score standardization\n",
    "scaler = StandardScaler()\n",
    "scaler_model = scaler.fit(train_X_numerical)\n",
    "\n",
    "train_X_num = scaler_model.transform(train_X_numerical)\n",
    "test_X_num = scaler_model.transform(test_X_numerical)\n",
    "\n",
    "# Encode categorical features\n",
    "encoder = OneHotEncoder(sparse_output=False, drop='if_binary')\n",
    "encoder_model = encoder.fit(train_X_categorical)\n",
    "train_X_categ = encoder.transform(train_X_categorical)\n",
    "test_X_categ = encoder.transform(test_X_categorical)\n",
    "\n",
    "# Recombine into single arrays\n",
    "train_X = np.concat([train_X_num, train_X_categ], axis=1)\n",
    "test_X = np.concat([test_X_num, test_X_categ], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "header-5",
   "metadata": {},
   "source": [
    "## 5. Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "context-9",
   "metadata": {},
   "source": [
    "### Training the Logistic Regression Model\n",
    "\n",
    "**What is happening:** Fitting the model to the training data.\n",
    "\n",
    "**The \"Why\":** **Logistic Regression** is a powerful baseline for classification, offering both reliability and interpretability for banking decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "cc914dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "\n",
    "# Perform model fitting\n",
    "# ravel() is used to convert target shape if warnings appear\n",
    "model.fit(train_X, train_Y.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "header-6",
   "metadata": {},
   "source": [
    "## 6. Evaluation & Results\n",
    "\n",
    "In the final phase, we use several metrics to measure how well our model predicts loan approvals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "context-10",
   "metadata": {},
   "source": [
    "### Evaluating Model Performance\n",
    "\n",
    "**What is happening:** Generating a detailed classification report.\n",
    "\n",
    "**The \"Why\":** Accuracy alone is insufficient. We need to monitor Precision and Recall to understand the trade-off between missing a good loan and approving a bad one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "24351a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      0.94      0.94       546\n",
      "         1.0       0.89      0.89      0.89       308\n",
      "\n",
      "    accuracy                           0.92       854\n",
      "   macro avg       0.91      0.91      0.91       854\n",
      "weighted avg       0.92      0.92      0.92       854\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_Y = model.predict(test_X)\n",
    "report = classification_report(test_Y, pred_Y)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "context-11",
   "metadata": {},
   "source": [
    "### Visualizing the Confusion Matrix\n",
    "\n",
    "**What is happening:** Plotting a heatmap of the classification errors.\n",
    "\n",
    "**The \"Why\":** It clarifies the distribution of True Positives, True Negatives, False Positives (Type I Error), and False Negatives (Type II Error)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "4f1d9056",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "...",
      "text/plain": [
       "<Figure size 1000x1000 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "con_mat = confusion_matrix(test_Y, pred_Y)\n",
    "plt.figure(figsize=(10, 10))\n",
    "heat_map = sns.heatmap(con_mat, annot=True, cmap='viridis', fmt='d')\n",
    "plt.title('Prediction Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "business-context",
   "metadata": {},
   "source": [
    "## 7. Business Context & Results Interpretation\n",
    "\n",
    "For financial stakeholders, the model's metrics translate directly into operational efficiency and risk management:\n",
    "\n",
    "### Precision and Credibility\n",
    "With a high **Precision** (e.g., ~0.94 for approved loans), the bank minimizes the risk of granting loans to high-risk candidates. This protects the bank's assets from default.\n",
    "\n",
    "### Recall and Opportunity\n",
    "A solid **Recall** implies that the bank is capturing a large percentage of creditworthy individuals. High recall means fewer legitimate customers are being rejected, ensuring the bank remains competitive and growth-oriented.\n",
    "\n",
    "### Business Recommendation\n",
    "The model shows strong performance with an accuracy of over 90%. However, continuous monitoring of **False Negatives** is recommended, as missing out on low-risk customers represents a loss in potential interest revenue. Future iterations could explore non-linear models like **Random Forests** to potentially improve the F1-score further."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
